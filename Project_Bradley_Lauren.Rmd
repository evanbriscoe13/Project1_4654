---
title: "Lauren_bradley_proj1"
author: "Lauren Bradley, Evan Briscoe"
date: "10/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(plyr)
library(caret)
```

# Main Goal: Create Loess Regression Function

```{r local_subset}
# Function that creates the windows 
create_subset <- function(obs, x, y, k){
  sub_df <- data.frame(x = x, y=y, dist = abs(obs-x))
  sub_df <- sub_df[order(sub_df$dist),]
  sub_df <- sub_df[1:k,]
  sub_df <- sub_df[order(sub_df$x),]
  return (sub_df)
}

```


```{r weight function}
# Function that creates the weights
triwts <- function(distances){
  #allocate weights vector
  wts <- double(length = length(distances))
  
  for (i in 1:length(distances)){
    if (abs(distances[i]) <= 1){
      wts[i] <- (1-abs(distances[i])**3)**3
    }
    else
      wts[i] <- 0
  }
  return (wts)
}
dist <- c(0.7866237, 0.3744029, 0.168412, 0.0000000, 0.6660479, 0.7498743, 1.0000000)
triwts(dist)
```

```{r}
# Your function will have the following inputs.
# 
# * x - a numeric input vector
# * y - a numeric response
#
# Note span and degree are shown with their default values. (Read about this in the description)
# * degree should be 1 or 2 only
# * span can be any value in interval (0, 1) non-inclusive.
#
# If show.plot = TRUE then you must show a plot of either the final fit
myloess <- function(x, y, span = 0.5, degree = 1, show.plot = TRUE){
  
  dat <- data.frame(x,y)
  # Your code goes here
  #N_total
  N_total <- length(x)
  #n_points
  n_points <- ceiling(length(y)*span)
  #Our fitted values vector
  y.fitted <- double(length = length(y))
  #past local subset
  past_loc_dat <- double(length = n_points)
  #Win_total
  Win_total = 0
  # Degree 2
  # index counter
  index = 1
  if(degree == 2){
    for (val in x){
      #Create current local subset vector
      loc_dat <- create_subset(val, x, y, n_points)
      #Compare to previous local subset
      # If previous local dataset is not equal to current
      if(!setequal(loc_dat$x, past_loc_dat)){
        Win_total = Win_total + 1
        past_loc_dat <- loc_dat$x
      }
      #calculate distance vector
      distances <- loc_dat$dist
      max.dist <- max(distances)
      #calculate scaled distances vector
      scale_dists <- distances/max.dist
      # call weight function to make vector
      wts <- triwts(scale_dists)
       #conduct weighted regression
      x_squared <- loc_dat$x^2
      fit <- lm(y~ x + x_squared, weights = wts, data = loc_dat)
      
      #calculate predicted value from regression fit and coefficients
      reg.value <- fit$coefficients[1] + fit$coefficients[2]*val +fit$coefficients[3]*val**2
      #Add to fitted values vector
      y.fitted[index] <- reg.value
      
      #increase index for next iteration
      index = index+1
    }
    
  }
  # Degree 1
  else{
    
    #Go through each point
    for (val in x){
      
      #Create current local subset vector
      loc_dat <- create_subset(val, x,y, n_points)
      #Compare to previous local subset
      # If previous local dataset is not equal to current
      if(!setequal(loc_dat$x, past_loc_dat)){
        Win_total = Win_total + 1
        past_loc_dat <- loc_dat$x
      }
      #calculate distance vector
      distances <- loc_dat$dist
      max.dist <- max(distances)
      #calculate scaled distances vector
      scale_dists <- distances/max.dist
      # call weight function to make vector
      wts <- triwts(scale_dists)
      #conduct weighted regression
      fit <- lm(y~ x, weights = wts, data = loc_dat)
      #calculate predicted value from regression fit and coefficients
      reg.value <- fit$coefficients[1] + fit$coefficients[2]*val
      #Add to fitted values vector
      y.fitted[index] <- reg.value
      
      #increase index for next iteration
      index = index+1
    }
    
  }
  SSE <- sum((y-y.fitted)**2)
  
  
  dat.fitted <- data.frame(x, y.fitted)
  loessplot <- ggplot(dat, aes(x = x, y = y)) + 
      theme_bw() + 
      geom_point() +
      geom_line(color='red', data = dat.fitted, aes(x = x, y = y.fitted))
      
  ls <- list('Span' = span, 'degree' = degree, 'N_total' = N_total, 'Win_total' = Win_total, 'n_points' = n_points, 'SSE' = SSE, 'loessplot' = loessplot)
  
  if (show.plot){
    return (ls)
  }
  else{
    return (ls[1:6])
  }
}
# Your function should return a named list containing the following:
# span: proportion of data used in each window (controls the bandwidth)
# degree: degree of polynomial
# N_total: total number of points in the data set
# Win_total: total number of windows
# n_points: number of points in each window in a vector
# SSE: Error Sum of Squares (Tells us how good of a fit we had).
# loessplot: An object containing the ggplot so that we can see the plot later. 
#  We want this even if show.plot = FALSE
#  Note: you are NOT allowed to simply use stat_smooth() or geom_smooth() to have it automatically do LOESS.
#  You should use geom_line() or similar to plot your final the LOESS curve.

# Make sure you can access the objects properly using the $ notation.
```

# Problem 1

```{r}
load("C:/Users/laure/git/Project1_4654/ozone.RData")

ggplot(ozone, aes(x = temperature, y = ozone)) + theme_bw() + geom_point()

```

## 1. 

```{r}
temp_2 <- ozone$temperature^2
temp_3 <- ozone$temperature^3
temp_4 <- ozone$temperature^4
temp_5 <- ozone$temperature^5
temp_6 <- ozone$temperature^6

poly1 <- lm(ozone ~ temperature, data = ozone)
poly2 <- lm(ozone ~ temperature + temp_2, data = ozone)
poly3 <- lm(ozone ~ temperature + temp_2 + temp_3, data = ozone)
poly4 <- lm(ozone ~ temperature + temp_2 + temp_3 + temp_4, data = ozone)
poly5 <- lm(ozone ~ temperature + temp_2 + temp_3 + temp_4 + temp_5, data = ozone)
poly6 <- lm(ozone ~ temperature + temp_2 + temp_3 + temp_4 + temp_5 + temp_6, data = ozone)

summary(poly1)
summary(poly2)
summary(poly3)
summary(poly4)
summary(poly5)
summary(poly6)


```
By looking at the regression summaries, the polynomial fit that appears to work the best is degree = 4

## 2. Loess Regression Fits

```{r}
tab_content <- c()
for (i in seq(0.25,0.75,.05)){
  degree_1 <- myloess(ozone$temperature, ozone$ozone, span = i, degree = 1, show.plot = FALSE)
  degree_2 <- myloess(ozone$temperature, ozone$ozone, span = i, degree = 2, show.plot = FALSE)
  
  tab_content <- append(tab_content, degree_1$SSE)
  tab_content <- append(tab_content, degree_2$SSE)
  tab_content <- append(tab_content, i)
}

table <- matrix(tab_content, ncol = 3, byrow = TRUE)
colnames(table) <- c('Degree1SSE', 'Degree2SSE', 'Span')
table <- as.table(table)
table
```

The best fits for Degree 1 are spans 0.25, 0.30, 0.35.

The best fits for Degree 2 are spans 0.25, 0.30, 0.35.

```{r}
myloess(ozone$temperature, ozone$ozone, span = 0.25, degree = 1, show.plot = TRUE)$loessplot + labs(title = "Degree 1 Span 0.25")
myloess(ozone$temperature, ozone$ozone, span = 0.30, degree = 1, show.plot = TRUE)$loessplot + labs(title = "Degree 1 Span 0.30")
myloess(ozone$temperature, ozone$ozone, span = 0.35, degree = 1, show.plot = TRUE)$loessplot + labs(title = "Degree 1 Span 0.35")
myloess(ozone$temperature, ozone$ozone, span = 0.25, degree = 2, show.plot = TRUE)$loessplot + labs(title = "Degree 2 Span 0.25")
myloess(ozone$temperature, ozone$ozone, span = 0.30, degree = 2, show.plot = TRUE)$loessplot + labs(title = "Degree 2 Span 0.30")
myloess(ozone$temperature, ozone$ozone, span = 0.35, degree = 2, show.plot = TRUE)$loessplot + labs(title = "Degree 2 Span 0.35")

```
Yes, we feel that the data is over fit for degree 1 and 2's span of 0.25 because the line is super jagged with unnecessary curves.  The best span with a smoother curve is 0.35 for both degree 1 and 2.

## 3. Compare with loess()
```{r}
mod1_25 <- loess(ozone ~ temperature, data = ozone, span = 0.25, degree = 1)
mod1_30 <- loess(ozone ~ temperature, data = ozone, span = 0.30, degree = 1)
mod1_35 <- loess(ozone ~ temperature, data = ozone, span = 0.35, degree = 1)
mod2_25 <- loess(ozone ~ temperature, data = ozone, span = 0.25, degree = 2)
mod2_30 <- loess(ozone ~ temperature, data = ozone, span = 0.30, degree = 2)
mod2_35 <- loess(ozone ~ temperature, data = ozone, span = 0.35, degree = 2)

print("Degree 1 SSE")
sum(mod1_25$residuals^2)
sum(mod1_30$residuals^2)
sum(mod1_35$residuals^2)

print("Degree 2 SSE")
sum(mod2_25$residuals^2)
sum(mod2_30$residuals^2)
sum(mod2_35$residuals^2)
```

```{r}
ggplot(ozone, aes(x = temperature, y = ozone)) + geom_point() + geom_smooth(method = "loess") + labs(title = "Built-in Loess")
```

The graphs from problem 2 have more precise and frequent curves with the manual loess function while the graph in problem 3 with the geom_smooth() function has a smoother curve. 

# Problem 2

```{r}
library(MASS)
data("mcycle")

ggplot(mcycle, aes(x = times, y = accel)) + theme_bw() + geom_point()
```

## 1.

```{r}
tab_content <- c()
for (i in seq(0.25,0.75,.05)){
  degree_1 <- myloess(mcycle$times, mcycle$accel, span = i, degree = 1, show.plot = FALSE)
  degree_2 <- myloess(mcycle$times, mcycle$accel, span = i, degree = 2, show.plot = FALSE)
  
  tab_content <- append(tab_content, degree_1$SSE)
  tab_content <- append(tab_content, degree_2$SSE)
  tab_content <- append(tab_content, i)
}

table <- matrix(tab_content, ncol = 3, byrow = TRUE)
colnames(table) <- c('Degree1SSE', 'Degree2SSE', 'Span')
table <- as.table(table)
table
```

The best fits for Degree 1 are spans 0.25, 0.30, 0.35.

The best fits for Degree 2 are spans 0.25, 0.30, 0.35.

```{r}
myloess(mcycle$times, mcycle$accel, span = 0.25, degree = 1, show.plot = TRUE)$loessplot + labs(title = "Degree 1 Span 0.25")
myloess(mcycle$times, mcycle$accel, span = 0.30, degree = 1, show.plot = TRUE)$loessplot + labs(title = "Degree 1 Span 0.30")
myloess(mcycle$times, mcycle$accel, span = 0.35, degree = 1, show.plot = TRUE)$loessplot + labs(title = "Degree 1 Span 0.35")
myloess(mcycle$times, mcycle$accel, span = 0.25, degree = 2, show.plot = TRUE)$loessplot + labs(title = "Degree 2 Span 0.25")
myloess(mcycle$times, mcycle$accel, span = 0.30, degree = 2, show.plot = TRUE)$loessplot + labs(title = "Degree 2 Span 0.30")
myloess(mcycle$times, mcycle$accel, span = 0.35, degree = 2, show.plot = TRUE)$loessplot + labs(title = "Degree 2 Span 0.35")
```

The degree 2 models have a better fit than the degree 1 models. Degree 2 with a span of 0.30 and 0.35 have the best model fits. 

## 2. 

```{r}
mod1_25 <- loess(times ~ accel, data = mcycle, span = 0.25, degree = 1)
mod1_30 <- loess(times ~ accel, data = mcycle, span = 0.30, degree = 1)
mod1_35 <- loess(times ~ accel, data = mcycle, span = 0.35, degree = 1)
mod2_25 <- loess(times ~ accel, data = mcycle, span = 0.25, degree = 2)
mod2_30 <- loess(times ~ accel, data = mcycle, span = 0.30, degree = 2)
mod2_35 <- loess(times ~ accel, data = mcycle, span = 0.35, degree = 2)

print("Degree 1 SSE")
sum(mod1_25$residuals^2)
sum(mod1_30$residuals^2)
sum(mod1_35$residuals^2)

print("Degree 2 SSE")
sum(mod2_25$residuals^2)
sum(mod2_30$residuals^2)
sum(mod2_35$residuals^2)
```


```{r}
ggplot(mcycle, aes(x = times, y = accel)) + geom_point() + geom_smooth(method = "loess") + labs(title = "Built-in Loess")
```


The manual function we made is better than the built in function.

# Main Goal: kNN

```{r distance}
#calculate the distance between our current point x
# each training point in the testing data set (features only)
calculateDistances <- function(newData, training_features) {
  D <- dist(rbind(newData, training_features)) # Compute the L2 norm by default
  D <- as.matrix(D)[-1, 1]
  names(D) <- 1:nrow(training_features)
  return(D) # Returns a vector of the distances
}
```


```{r}
# Your function will have the following inputs similar to what you would find with the
#  knn() function
#
# * train - matrix or data frame of training set cases
# * test - matrix or data frame of test set cases.  
#     (A vector will be interpreted as a row vector for a single case.)
# * y_train - Either a numeric vector, or factor vector for the responses in the training set
# * y_test - Either a numeric vector, or factor vector for the responses in the testing set
# * k - number of neighbors considered, the default value is 3
#
# If weighted = TRUE, then your function must used the distance weighted kNN as described above,
#  otherwise it should do the default knn method.
library(class)

mykNN <- function(train, test, y_train, y_test, k = 3, weighted = TRUE){
  y_hat <- c()
  
  # Your code goes here
  # Do kNN instead of weighted
  if (weighted == FALSE){
    
    # Classification
    if(is.factor(y_test)){
      for (i in 1:dim(test)[1]){
        dists <- calculateDistances(test[i,], train)
        o <- order(dists, decreasing = FALSE)
        index <- seq_along(dists)
        # Grab k closest distances and indexes
        dists_k <- cbind(dists[o],index[o])[1:k,]
        if (k == 1){
          dist_k_idx <- dists_k[2]
        }
        else {
          dist_k_idx <- dists_k[,2]
        }
        find_lab <- data.frame(Label = y_train[dist_k_idx], dist = dists_k[,1])
        find_lab <- find_lab[order(find_lab$dist, decreasing = TRUE),]
        label <- find_lab[1,1]
        y_hat <- append(y_hat, label)
      }
      tb <- table(y_hat, y_test)
      accuracy <- sum(diag(tb)/sum(rowSums(tb)))
      error_rate <- 1 - accuracy
      conf <- confusionMatrix(data = y_hat, reference = y_test)
      ls <- list('yhat' = y_hat, 'accuracy' = accuracy, 'error_rate' = error_rate, 'confusion_matrix' = conf, 'k' = k)
    
      return (ls)
      
    }
    # Regression
    else {
      for (i in 1:dim(test)[1]){
        dists <- calculateDistances(test[i,], train)
        index <- seq_along(dists)
        o <- order(dists, decreasing = FALSE)
        # Grab k closest distances and indexes
        dists_k <- cbind(dists[o],index[o])[1:k,]
        if (k == 1){
          dist_k_idx <- dists_k[2]
        }
        else {
          dist_k_idx <- dists_k[,2]
        }
        
        fitted_value <- c()
        for (x in seq_along(k)){
          fitted_value <- append(fitted_value, y_train[dist_k_idx])
        }
        y_hat <- append(y_hat, (1/k)*sum(fitted_value))
      }
      res_vec <- y_test - unlist(y_hat)
      SSE <- sum((y_test-unlist(y_hat))^2)
      ls <- list("yhat" = y_hat, "residual_vector" = res_vec, "SSE" = SSE, "k" = k)
      return (ls)
    }
    
    
  }
    
    
    # If responses are factor vector (classification)
    # If you are doing classification, then your function must return:
    #  * A factor vector (yhat) for the predicted categories for the testing data
    #  * The accuracy of the classification
    #  * The error rate = 1 - accuracy
    #  * A confusion matrix
    #  * The value of k used
  else if( is.factor(y_test)){
      # Go through each row (entry) of test 
    for (i in 1:dim(test)[1]){
      # Get distances of test row with all training rows
      dists <- calculateDistances(test[i,], train)
      index <- seq_along(dists)
      o <- order(dists, decreasing = FALSE)
      # Grab k closest distances and indexes
      dists_k <- cbind(dists[o],index[o])[1:k,]
      if (k == 1){
        weights <- 1/dists_k[1]
        dist_k_idx <- dists_k[2]
      }
      else{
        weights <- 1/dists_k[,1]
        dist_k_idx <- dists_k[,2]
      }
      

      weight_sum <- data.frame(Label = y_train[dist_k_idx], weight = weights)
      weight_sum <- aggregate(.~Label, data = weight_sum, FUN = sum)
      weight_sum <- weight_sum[order(weight_sum$weight, decreasing = TRUE),]
      label <- weight_sum[1,1]
      y_hat <- append(y_hat, label)
    }
    tb <- table(y_hat, y_test)
    accuracy <- sum(diag(tb)/sum(rowSums(tb)))
    error_rate <- 1 - accuracy
    conf <- confusionMatrix(data = y_hat, reference = y_test)
    
    ls <- list('yhat' = y_hat, 'accuracy' = accuracy, 'error_rate' = error_rate, 'confusion_matrix' = conf, 'k' = k)
    
    return (ls)

  }
  # Responses are number vector (Regression)
  # If you are doing regression, then your function must return:
  #  * A numeric vector (yhat) for the predicted responses for the testing data
  #  * The residual vector
  #  * The SSE
  #  * The value of k used
  else {
    # Go through each row (entry) of test 
    for (i in 1:dim(test)[1]){
      # Get distances of test row with all training rows
      dists <- calculateDistances(test[i,], train)
      index <- seq_along(dists)
      o <- order(dists, decreasing = FALSE)
      # Grab k closest distances and indexes
      dists_k <- cbind(dists[o],index[o])[1:k,]
      if (k == 1){
        weights <- 1/dists_k[1]
        dist_k_idx <- dists_k[2]
      }
      else{
        weights <- 1/dists_k[, 1]
        dist_k_idx <- dists_k[, 2]
      }
      c <- weights/sum(weights)
      fitted_value <- c()
      for (x in seq_along(k)){
        fitted_value <- append(fitted_value, (c[x]*y_train[dist_k_idx]))
      }
      y_hat <- append(y_hat, sum(fitted_value))
    }
    res_vec <- y_test - unlist(y_hat)
    SSE <- sum((y_test-unlist(y_hat))^2)
    ls <- list("yhat" = y_hat, "residual_vector" = res_vec, "SSE" = SSE, "k" = k)
    return (ls)
  }
}

```

# Problem 3

```{r}
# Some pre-processing
library(ISLR)
# Remove the name of the car model and change the origin to categorical with actual name
Auto_new <- Auto[, -9]
# Lookup table
newOrigin <- c("USA", "European", "Japanese")
Auto_new$origin <- factor(newOrigin[Auto_new$origin], newOrigin)

# Look at the first 6 observations to see the final version
head(Auto_new)
```

## 1. Randomly split data into two data frames

```{r}
index <- sample(1:nrow(Auto_new), round(nrow(Auto_new) * 0.7))
training_df <- Auto_new[index, ]
testing_df <- Auto_new[-index, ]

```

```{r}
# Store the training/testing data features
train_features <- training_df[, 1:dim(training_df)[2] - 1]
test_features <- testing_df[, 1:dim(training_df)[2] - 1]
# Store the actual labels
train_classes <- training_df$origin
test_classes <- testing_df$origin
```

## Regular kNN

## 2. Use training and testing data 

```{r}
reg_kNN2 <- mykNN(train_features, test_features, train_classes, test_classes, k = 2, weighted = FALSE)
reg_kNN3 <- mykNN(train_features, test_features, train_classes, test_classes, k = 3, weighted = FALSE)
reg_kNN4 <- mykNN(train_features, test_features, train_classes, test_classes, k = 4, weighted = FALSE)
reg_kNN5 <- mykNN(train_features, test_features, train_classes, test_classes, k = 5, weighted = FALSE)
reg_kNN6 <- mykNN(train_features, test_features, train_classes, test_classes, k = 6, weighted = FALSE)
reg_kNN7 <- mykNN(train_features, test_features, train_classes, test_classes, k = 7, weighted = FALSE)
reg_kNN8 <- mykNN(train_features, test_features, train_classes, test_classes, k = 8, weighted = FALSE)
reg_kNN9 <- mykNN(train_features, test_features, train_classes, test_classes, k = 9, weighted = FALSE)
reg_kNN10 <- mykNN(train_features, test_features, train_classes, test_classes, k = 10, weighted = FALSE)
```

```{r}
print(reg_kNN3$accuracy)
print(reg_kNN5$accuracy)
print(reg_kNN7$accuracy)
print(reg_kNN10$accuracy)
```
## 3. Make table of accuracy

```{r}
tab_vals <- c(2, reg_kNN2$accuracy ,3,reg_kNN3$accuracy, 4, reg_kNN4$accuracy, 5, reg_kNN5$accuracy, 6, reg_kNN6$accuracy, 7, reg_kNN7$accuracy, 8, reg_kNN8$accuracy, 9, reg_kNN9$accuracy, 10, reg_kNN10$accuracy)
table <- matrix(tab_vals, ncol = 2, byrow = TRUE)
colnames(table) <- c('k', 'Accuracy')
table <- as.table(table)
knitr::kable(table, col.names = c("k", "Accuracy"))

```

## 4. Make a plot of the accuracy vs. k

```{r}
k <- c(2, 3, 4, 5, 6, 7, 8, 9, 10)
accuracy <- c(reg_kNN2$accuracy, reg_kNN3$accuracy, reg_kNN4$accuracy ,reg_kNN5$accuracy, reg_kNN6$accuracy,reg_kNN7$accuracy, reg_kNN8$accuracy ,reg_kNN9$accuracy,reg_kNN10$accuracy)
#df <- data.frame(table)
plot(k, accuracy, main = "Accuracy vs. K", pch = 16)
```

It looks like k = 3 is the best number of neighbors. 

## 5. Final confusion matrix of k = 3

```{r}
reg_kNN3$confusion_matrix
```
With k = 3, the k nearest neighbors is 69.45% accurate.

## Graphing 

```{r}
ggplot(train_features, aes(x = weight, y = mpg, color = train_classes)) + geom_point(alpha = 0.5) + ggtitle("MPG vs. Weight k = 5") + geom_point(data = test_features, aes(weight, mpg, color = reg_kNN5$yhat), shape = 18)
```


```{r}
ggplot(train_features, aes(x = weight, y = mpg, color = train_classes)) + geom_point(alpha = 0.5) + ggtitle("MPG vs. Weight k = 10") + geom_point(data = test_features, aes(weight, mpg, color = reg_kNN10$yhat), shape = 18)
```

# dnkNN

## 2. 

```{r}
wt_kNN2 <- mykNN(train_features, test_features, train_classes, test_classes, k = 2, weighted = TRUE)
wt_kNN3 <- mykNN(train_features, test_features, train_classes, test_classes, k = 3, weighted = TRUE)
wt_kNN4 <- mykNN(train_features, test_features, train_classes, test_classes, k = 4, weighted = TRUE)
wt_kNN5 <- mykNN(train_features, test_features, train_classes, test_classes, k = 5, weighted = TRUE)
wt_kNN6 <- mykNN(train_features, test_features, train_classes, test_classes, k = 6, weighted = TRUE)
wt_kNN7 <- mykNN(train_features, test_features, train_classes, test_classes, k = 7, weighted = TRUE)
wt_kNN8 <- mykNN(train_features, test_features, train_classes, test_classes, k = 8, weighted = TRUE)
wt_kNN9 <- mykNN(train_features, test_features, train_classes, test_classes, k = 9, weighted = TRUE)
wt_kNN10 <- mykNN(train_features, test_features, train_classes, test_classes, k = 10, weighted = TRUE)
```

```{r}
print(wt_kNN3$accuracy)
print(wt_kNN5$accuracy)
print(wt_kNN7$accuracy)
print(wt_kNN10$accuracy)
```
## 3. Make table of accuracy

```{r}
tab_vals <- c(2, wt_kNN2$accuracy ,3,wt_kNN3$accuracy, 4, wt_kNN4$accuracy, 5, wt_kNN5$accuracy, 6, wt_kNN6$accuracy, 7, wt_kNN7$accuracy, 8, wt_kNN8$accuracy, 9, wt_kNN9$accuracy, 10, wt_kNN10$accuracy)
table <- matrix(tab_vals, ncol = 2, byrow = TRUE)
colnames(table) <- c('k', 'Accuracy')
table <- as.table(table)
knitr::kable(table, col.names = c("k", "Accuracy"))

```

## 4. Plot accuracy vs. K

```{r}
k <- c(2, 3, 4, 5, 6, 7, 8, 9, 10)
accuracy <- c(wt_kNN2$accuracy, wt_kNN3$accuracy, wt_kNN4$accuracy ,wt_kNN5$accuracy, wt_kNN6$accuracy,wt_kNN7$accuracy, wt_kNN8$accuracy ,wt_kNN9$accuracy,wt_kNN10$accuracy)
#df <- data.frame(table)
plot(k, accuracy, main = "Accuracy vs. K", pch = 16)
```

Using 3 of the neighbors looks the best. 

## 5. Display confusion matrix of k = 10

```{r}
wt_kNN10$confusion_matrix
```
Using k = 10, there is an accuracy of 67.8%

Comparing the weighted and regular knn with classification, as you increase the amount of neighbors for the weighted knn, there is an increase of accuracy. As you increase the amount of neighbors for the regular knn, there is a decrease of accuracy. 

## Graphing data

```{r}
ggplot(train_features, aes(x = weight, y = mpg, color = train_classes)) + geom_point(alpha = 0.5) + ggtitle("MPG vs. Weight k = 5") + geom_point(data = test_features, aes(weight, mpg, color = wt_kNN5$yhat), shape = 18)
```

```{r}
ggplot(train_features, aes(x = weight, y = mpg, color = train_classes)) + geom_point(alpha = 0.5) + ggtitle("MPG vs. Weight k = 10") + geom_point(data = test_features, aes(weight, mpg, color = wt_kNN10$yhat), shape = 18)
```

# Problem 4

```{r}
index <- sample(1:nrow(ozone), 70)
training_df <- ozone[index, ]
testing_df <- ozone[-index, ]
```

```{r}
# Store the training/testing data features
train_features <- training_df[, 2:dim(training_df)[2]]
test_features <- testing_df[, 2:dim(training_df)[2]]
# Store the actual labels
train_classes <- training_df$ozone
test_classes <- testing_df$ozone
```

## Part a


```{r}
k <- c(1,3,5,10,20)

tab_vals <- c()

for (i in k){
  dwkNN <- mykNN(train_features, test_features, train_classes, test_classes, k = i, weighted = TRUE)
  
  print(ggplot(training_df, aes(temperature, ozone)) 
        + geom_point(color = "black") 
        + geom_point(data = testing_df, aes(temperature, ozone), color = "blue") 
        + geom_line(data = testing_df, aes(temperature, dwkNN$yhat, color = "red"))
        + geom_point(data = testing_df, aes(temperature, dwkNN$yhat, color = "red")) + ggtitle(i))
  
  tab_vals <- append(tab_vals, i)
  tab_vals <- append(tab_vals, dwkNN$SSE)
}
```


```{r}
table <- matrix(tab_vals, ncol = 2, byrow = TRUE)
colnames(table) <- c('k', 'SSE')
table <- as.table(table)
knitr::kable(table)
```
The lease error occurs at k = 1. In the plot, it also looks that the regression is more accurate and doesn't have many bumps. 

## Part b.

```{r}
plot_vals <- c()
for (i in 1:20){
  dwkNN <- mykNN(train_features, test_features, train_classes, test_classes, k = i, weighted = TRUE)
  
  plot_vals <- append(plot_vals, dwkNN$SSE)
}
```


```{r}
k <- 1:20
plot(k, plot_vals, main = "SSE vs. K", ylab = "SSE", pch = 16)
```

As k increases, the sum of squared errors increases. The k and SSE have a strong positive linear regression. Therefore a smaller k will have the best regression prediction. 